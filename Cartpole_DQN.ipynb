{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s2L7gOiDrsx-"
   },
   "source": [
    "# Acknowledgement\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "klGNgWREsvQv"
   },
   "source": [
    "##### Copyright 2018 The TF-Agents Authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T16:08:40.229553Z",
     "start_time": "2019-11-13T16:08:40.227350Z"
    },
    "cellView": "both",
    "colab": {},
    "colab_type": "code",
    "id": "nQnmcm0oI1Q-"
   },
   "outputs": [],
   "source": [
    "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# https://www.apache.org/licenses/LICENSE-2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BMVXRvomsC2a"
   },
   "source": [
    "This notebook is based on the official TF-Agents DQN tutorial ipython notebook.\n",
    "\n",
    "reference: https://github.com/tensorflow/agents/blob/master/tf_agents/colabs/1_dqn_tutorial.ipynb\n",
    "\n",
    "Also, see the official github repo for more information: https://github.com/tensorflow/agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pmDI-h7cI0tI"
   },
   "source": [
    "# Cartpole Example - Train a Deep Q Network with TF-Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lsaQlK8fFQqH"
   },
   "source": [
    "### Get Started\n",
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/agents/blob/master/tf_agents/colabs/1_dqn_tutorial.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://github.com/tensorflow/agents/blob/master/tf_agents/colabs/1_dqn_tutorial.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
    "  </td>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cKOCZlhUgXVK"
   },
   "source": [
    "This example shows how to train a [DQN (Deep Q Networks)](https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf)  agent on the Cartpole environment using the TF-Agents library.\n",
    "\n",
    "![Cartpole environment](https://raw.githubusercontent.com/tensorflow/agents/master/tf_agents/colabs/images/cartpole.png)\n",
    "\n",
    "It will walk you through all the components in a Reinforcement Learning (RL) pipeline for training, evaluation and data collection.\n",
    "\n",
    "\n",
    "To run this code live, click the 'Run in Google Colab' link above.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1u9QVVsShC9X"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T16:09:15.774365Z",
     "start_time": "2019-11-13T16:09:02.290931Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "KEHR2Ui-lo8O"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: apt-get: command not found\n",
      "Collecting gym==0.10.11\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/87/04/70d4901b7105082c9742acd64728342f6da7cd471572fd0660a73f9cfe27/gym-0.10.11.tar.gz (1.5MB)\n",
      "\u001b[K     |████████████████████████████████| 1.5MB 5.5MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: scipy in /Users/msyeom/anaconda3/envs/dl/lib/python3.6/site-packages (from gym==0.10.11) (1.3.0)\n",
      "Requirement already satisfied: numpy>=1.10.4 in /Users/msyeom/anaconda3/envs/dl/lib/python3.6/site-packages (from gym==0.10.11) (1.17.2)\n",
      "Collecting requests>=2.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/51/bd/23c926cd341ea6b7dd0b2a00aba99ae0f828be89d72b2190f27c11d4b7fb/requests-2.22.0-py2.py3-none-any.whl (57kB)\n",
      "\u001b[K     |████████████████████████████████| 61kB 16.4MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six in /Users/msyeom/anaconda3/envs/dl/lib/python3.6/site-packages (from gym==0.10.11) (1.12.0)\n",
      "Requirement already satisfied: pyglet>=1.2.0 in /Users/msyeom/anaconda3/envs/dl/lib/python3.6/site-packages (from gym==0.10.11) (1.3.2)\n",
      "Collecting idna<2.9,>=2.5\n",
      "  Using cached https://files.pythonhosted.org/packages/14/2c/cd551d81dbe15200be1cf41cd03869a46fe7226e7450af7a6545bfc474c9/idna-2.8-py2.py3-none-any.whl\n",
      "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b4/40/a9837291310ee1ccc242ceb6ebfd9eb21539649f193a7c8c86ba15b98539/urllib3-1.25.7-py2.py3-none-any.whl (125kB)\n",
      "\u001b[K     |████████████████████████████████| 133kB 12.2MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /Users/msyeom/anaconda3/envs/dl/lib/python3.6/site-packages (from requests>=2.0->gym==0.10.11) (2019.6.16)\n",
      "Collecting chardet<3.1.0,>=3.0.2\n",
      "  Using cached https://files.pythonhosted.org/packages/bc/a9/01ffebfb562e4274b6487b4bb1ddec7ca55ec7510b22e4c51f14098443b8/chardet-3.0.4-py2.py3-none-any.whl\n",
      "Requirement already satisfied: future in /Users/msyeom/anaconda3/envs/dl/lib/python3.6/site-packages (from pyglet>=1.2.0->gym==0.10.11) (0.18.1)\n",
      "Building wheels for collected packages: gym\n",
      "  Building wheel for gym (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for gym: filename=gym-0.10.11-cp36-none-any.whl size=1588314 sha256=6384ce682b963499ecad5c6eccc4d85ce780537043e935de955e145cf4fff558\n",
      "  Stored in directory: /Users/msyeom/Library/Caches/pip/wheels/7b/eb/1f/22c4124f3c64943aa0646daf4612b1c1f00f27d89b81304ebd\n",
      "Successfully built gym\n",
      "Installing collected packages: idna, urllib3, chardet, requests, gym\n",
      "  Found existing installation: gym 0.15.3\n",
      "    Uninstalling gym-0.15.3:\n",
      "      Successfully uninstalled gym-0.15.3\n",
      "Successfully installed chardet-3.0.4 gym-0.10.11 idna-2.8 requests-2.22.0 urllib3-1.25.7\n",
      "Collecting imageio==2.4.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ac/64/8e2bb6aac43d6ed7c2d9514320b43d5e80c00f150ee2b9408aee24359e6d/imageio-2.4.0.tar.gz (3.3MB)\n",
      "\u001b[K     |████████████████████████████████| 3.3MB 7.9MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy in /Users/msyeom/anaconda3/envs/dl/lib/python3.6/site-packages (from imageio==2.4.0) (1.17.2)\n",
      "Requirement already satisfied: pillow in /Users/msyeom/anaconda3/envs/dl/lib/python3.6/site-packages (from imageio==2.4.0) (6.1.0)\n",
      "Building wheels for collected packages: imageio\n",
      "  Building wheel for imageio (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for imageio: filename=imageio-2.4.0-cp36-none-any.whl size=3303880 sha256=5c3e6c06454defe71a097dc904b75676ba88c6efa8340531c923d531d50ac11a\n",
      "  Stored in directory: /Users/msyeom/Library/Caches/pip/wheels/31/83/88/a1cba54ac06395d9e4ddcd9cf06911cd0b26cd78af9a61071b\n",
      "Successfully built imageio\n",
      "Installing collected packages: imageio\n",
      "Successfully installed imageio-2.4.0\n",
      "Requirement already satisfied: PILLOW in /Users/msyeom/anaconda3/envs/dl/lib/python3.6/site-packages (6.1.0)\n",
      "Requirement already satisfied: pyglet==1.3.2 in /Users/msyeom/anaconda3/envs/dl/lib/python3.6/site-packages (1.3.2)\n",
      "Requirement already satisfied: future in /Users/msyeom/anaconda3/envs/dl/lib/python3.6/site-packages (from pyglet==1.3.2) (0.18.1)\n",
      "Collecting pyvirtualdisplay\n",
      "  Downloading https://files.pythonhosted.org/packages/cf/ad/b15f252bfb0f1693ad3150b55a44a674f3cba711cacdbb9ae2f03f143d19/PyVirtualDisplay-0.2.4-py2.py3-none-any.whl\n",
      "Collecting EasyProcess\n",
      "  Downloading https://files.pythonhosted.org/packages/fa/29/40040d1d64a224a5e44df9572794a66494618ffe5c77199214aeceedb8a7/EasyProcess-0.2.7-py2.py3-none-any.whl\n",
      "Installing collected packages: EasyProcess, pyvirtualdisplay\n",
      "Successfully installed EasyProcess-0.2.7 pyvirtualdisplay-0.2.4\n",
      "Requirement already satisfied: tf-agents-nightly in /Users/msyeom/.local/lib/python3.6/site-packages (0.2.0.dev20191104)\n",
      "Requirement already satisfied: tfp-nightly in /Users/msyeom/.local/lib/python3.6/site-packages (from tf-agents-nightly) (0.9.0.dev20191104)\n",
      "Requirement already satisfied: gin-config==0.1.3 in /Users/msyeom/.local/lib/python3.6/site-packages (from tf-agents-nightly) (0.1.3)\n",
      "Requirement already satisfied: absl-py>=0.6.1 in /Users/msyeom/anaconda3/envs/dl/lib/python3.6/site-packages (from tf-agents-nightly) (0.8.1)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /Users/msyeom/anaconda3/envs/dl/lib/python3.6/site-packages (from tf-agents-nightly) (1.17.2)\n",
      "Requirement already satisfied: six>=1.10.0 in /Users/msyeom/anaconda3/envs/dl/lib/python3.6/site-packages (from tf-agents-nightly) (1.12.0)\n",
      "Requirement already satisfied: decorator in /Users/msyeom/anaconda3/envs/dl/lib/python3.6/site-packages (from tfp-nightly->tf-agents-nightly) (4.4.0)\n",
      "Requirement already satisfied: gast>=0.2 in /Users/msyeom/anaconda3/envs/dl/lib/python3.6/site-packages (from tfp-nightly->tf-agents-nightly) (0.2.2)\n",
      "Requirement already satisfied: cloudpickle>=1.2.2 in /Users/msyeom/anaconda3/envs/dl/lib/python3.6/site-packages (from tfp-nightly->tf-agents-nightly) (1.2.2)\n"
     ]
    }
   ],
   "source": [
    "# Note: If you haven't installed the following dependencies, run:\n",
    "!apt-get install xvfb\n",
    "!pip install 'gym==0.10.11'\n",
    "!pip install 'imageio==2.4.0'\n",
    "!pip install PILLOW\n",
    "!pip install 'pyglet==1.3.2'\n",
    "!pip install pyvirtualdisplay\n",
    "!pip install tf-agents-nightly\n",
    "try:\n",
    "  %%tensorflow_version 2.x\n",
    "except:\n",
    "  pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T16:09:37.912383Z",
     "start_time": "2019-11-13T16:09:20.394186Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "sMitx5qSgJk1"
   },
   "outputs": [
    {
     "ename": "XStartTimeoutError",
     "evalue": "Failed to start X on display \":1001\" (xdpyinfo check failed).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mXStartTimeoutError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-af038be80e9a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;31m# Set up a virtual display for rendering OpenAI gym environments.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0mdisplay\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpyvirtualdisplay\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvisible\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1400\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m900\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/dl/lib/python3.6/site-packages/pyvirtualdisplay/abstractdisplay.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    166\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mok\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Failed to start X on display \"%s\" (xdpyinfo check failed).'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mXStartTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mXStartTimeoutError\u001b[0m: Failed to start X on display \":1001\" (xdpyinfo check failed)."
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import base64\n",
    "import imageio\n",
    "import IPython\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL.Image\n",
    "import pyvirtualdisplay\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tf_agents.agents.dqn import dqn_agent\n",
    "from tf_agents.drivers import dynamic_step_driver\n",
    "from tf_agents.environments import suite_gym\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.eval import metric_utils\n",
    "from tf_agents.metrics import tf_metrics\n",
    "from tf_agents.networks import q_network\n",
    "from tf_agents.policies import random_tf_policy\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.trajectories import trajectory\n",
    "from tf_agents.utils import common\n",
    "\n",
    "\n",
    "\n",
    "tf.compat.v1.enable_v2_behavior()\n",
    "\n",
    "\n",
    "# Set up a virtual display for rendering OpenAI gym environments.\n",
    "display = pyvirtualdisplay.Display(visible=0, size=(1400, 900)).start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T16:13:46.713951Z",
     "start_time": "2019-11-13T16:13:46.708614Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "NspmzG4nP3b9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.0'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.version.VERSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LmC0NDhdLIKY"
   },
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T16:13:48.310915Z",
     "start_time": "2019-11-13T16:13:48.307392Z"
    },
    "cellView": "both",
    "colab": {},
    "colab_type": "code",
    "id": "HC1kNrOsLSIZ"
   },
   "outputs": [],
   "source": [
    "num_iterations = 20000 # @param {type:\"integer\"}\n",
    "\n",
    "initial_collect_steps = 1000  # @param {type:\"integer\"} \n",
    "collect_steps_per_iteration = 1  # @param {type:\"integer\"}\n",
    "replay_buffer_max_length = 100000  # @param {type:\"integer\"}\n",
    "\n",
    "batch_size = 64  # @param {type:\"integer\"}\n",
    "learning_rate = 1e-3  # @param {type:\"number\"}\n",
    "log_interval = 200  # @param {type:\"integer\"}\n",
    "\n",
    "num_eval_episodes = 10  # @param {type:\"integer\"}\n",
    "eval_interval = 1000  # @param {type:\"integer\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VMsJC3DEgI0x"
   },
   "source": [
    "## Environment\n",
    "\n",
    "In Reinforcement Learning (RL), an environment represents the task or problem to be solved. Standard environments can be created in TF-Agents using `tf_agents.environments` suites. TF-Agents has suites for loading environments from sources such as the OpenAI Gym, Atari, and DM Control.\n",
    "\n",
    "Load the CartPole environment from the OpenAI Gym suite. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T16:13:53.592421Z",
     "start_time": "2019-11-13T16:13:53.582255Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "pYEz-S9gEv2-"
   },
   "outputs": [],
   "source": [
    "env_name = 'CartPole-v0'\n",
    "env = suite_gym.load(env_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IIHYVBkuvPNw"
   },
   "source": [
    "You can render this environment to see how it looks. A free-swinging pole is attached to a cart.  The goal is to move the cart right or left in order to keep the pole pointing up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T16:13:59.179968Z",
     "start_time": "2019-11-13T16:13:56.528221Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "RlO7WIQHu_7D"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=1200x800 at 0x13EFEDD30>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#@test {\"skip\": true}\n",
    "env.reset()\n",
    "PIL.Image.fromarray(env.render())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B9_lskPOey18"
   },
   "source": [
    "The `environment.step` method takes an `action` in the environment and returns a `TimeStep` tuple containing the next observation of the environment and the reward for the action.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eJCgJnx3g0yY"
   },
   "source": [
    "In the Cartpole environment:\n",
    "\n",
    "-   `observation` is an array of 4 floats: \n",
    "    -   the position and velocity of the cart\n",
    "    -   the angular position and velocity of the pole \n",
    "-   `reward` is a scalar float value\n",
    "-   `action` is a scalar integer with only two possible values:\n",
    "    -   `0` — \"move left\"\n",
    "    -   `1` — \"move right\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T16:14:43.394729Z",
     "start_time": "2019-11-13T16:14:43.389199Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "V2UGR5t_iZX-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time step:\n",
      "TimeStep(step_type=array(0, dtype=int32), reward=array(0., dtype=float32), discount=array(1., dtype=float32), observation=array([-0.02830465, -0.04743285, -0.0088459 ,  0.02510164], dtype=float32))\n",
      "Next time step:\n",
      "TimeStep(step_type=array(1, dtype=int32), reward=array(1., dtype=float32), discount=array(1., dtype=float32), observation=array([-0.02925331,  0.14781484, -0.00834386, -0.27035907], dtype=float32))\n"
     ]
    }
   ],
   "source": [
    "time_step = env.reset()\n",
    "print('Time step:')\n",
    "print(time_step)\n",
    "\n",
    "action = 1\n",
    "\n",
    "next_time_step = env.step(action)\n",
    "print('Next time step:')\n",
    "print(next_time_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4JSc9GviWUBK"
   },
   "source": [
    "Usually two environments are instantiated: one for training and one for evaluation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T16:15:35.460526Z",
     "start_time": "2019-11-13T16:15:35.455119Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "N7brXNIGWXjC"
   },
   "outputs": [],
   "source": [
    "train_py_env = suite_gym.load(env_name)\n",
    "eval_py_env = suite_gym.load(env_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zuUqXAVmecTU"
   },
   "source": [
    "The Cartpole environment, like most environments, is written in pure Python. This is converted to TensorFlow using the `TFPyEnvironment` wrapper.\n",
    "\n",
    "The original environment's API uses Numpy arrays. The `TFPyEnvironment` converts these to `Tensors` to make it compatible with Tensorflow agents and policies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T16:15:52.784686Z",
     "start_time": "2019-11-13T16:15:52.771285Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "Xp-Y4mD6eDhF"
   },
   "outputs": [],
   "source": [
    "train_env = tf_py_environment.TFPyEnvironment(train_py_env)\n",
    "eval_env = tf_py_environment.TFPyEnvironment(eval_py_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T16:15:59.356048Z",
     "start_time": "2019-11-13T16:15:59.351797Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf_agents.environments.wrappers.TimeLimit at 0x106105e10>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_py_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T16:16:05.835823Z",
     "start_time": "2019-11-13T16:16:05.831883Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf_agents.environments.tf_py_environment.TFPyEnvironment at 0x1408feef0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E9lW_OZYFR8A"
   },
   "source": [
    "## Agent\n",
    "\n",
    "The algorithm used to solve an RL problem is represented by an `Agent`. TF-Agents provides standard implementations of a variety of `Agents`, including:\n",
    "\n",
    "-   [DQN](https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf) (used in this tutorial)\n",
    "-   [REINFORCE](http://www-anw.cs.umass.edu/~barto/courses/cs687/williams92simple.pdf)\n",
    "-   [DDPG](https://arxiv.org/pdf/1509.02971.pdf)\n",
    "-   [TD3](https://arxiv.org/pdf/1802.09477.pdf)\n",
    "-   [PPO](https://arxiv.org/abs/1707.06347)\n",
    "-   [SAC](https://arxiv.org/abs/1801.01290).\n",
    "\n",
    "The DQN agent can be used in any environment which has a discrete action space.\n",
    "\n",
    "At the heart of a DQN Agent is a `QNetwork`, a neural network model that can learn to predict `QValues` (expected returns) for all actions, given an observation from the environment.\n",
    "\n",
    "Use `tf_agents.networks.q_network` to create a `QNetwork`, passing in the `observation_spec`, `action_spec`, and a tuple describing the number and size of the model's hidden layers.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T16:18:28.314491Z",
     "start_time": "2019-11-13T16:18:28.280586Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "TgkdEPg_muzV"
   },
   "outputs": [],
   "source": [
    "fc_layer_params = (100,)\n",
    "\n",
    "q_net = q_network.QNetwork(\n",
    "    train_env.observation_spec(),\n",
    "    train_env.action_spec(),\n",
    "    fc_layer_params=fc_layer_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z62u55hSmviJ"
   },
   "source": [
    "Now use `tf_agents.agents.dqn.dqn_agent` to instantiate a `DqnAgent`. In addition to the `time_step_spec`, `action_spec` and the QNetwork, the agent constructor also requires an optimizer (in this case, `AdamOptimizer`), a loss function, and an integer step counter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T16:18:29.122339Z",
     "start_time": "2019-11-13T16:18:29.036575Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "jbY4yrjTEyc9"
   },
   "outputs": [],
   "source": [
    "optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "train_step_counter = tf.Variable(0)\n",
    "\n",
    "agent = dqn_agent.DqnAgent(\n",
    "    train_env.time_step_spec(),\n",
    "    train_env.action_spec(),\n",
    "    q_network=q_net,\n",
    "    optimizer=optimizer,\n",
    "    td_errors_loss_fn=common.element_wise_squared_loss,\n",
    "    train_step_counter=train_step_counter)\n",
    "\n",
    "agent.initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I0KLrEPwkn5x"
   },
   "source": [
    "## Policies\n",
    "\n",
    "A policy defines the way an agent acts in an environment. Typically, the goal of reinforcement learning is to train the underlying model until the policy produces the desired outcome.\n",
    "\n",
    "In this tutorial:\n",
    "\n",
    "-   The desired outcome is keeping the pole balanced upright over the cart.\n",
    "-   The policy returns an action (left or right) for each `time_step` observation.\n",
    "\n",
    "Agents contain two policies: \n",
    "\n",
    "-   `agent.policy` — The main policy that is used for evaluation and deployment.\n",
    "-   `agent.collect_policy` — A second policy that is used for data collection.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T16:19:14.284913Z",
     "start_time": "2019-11-13T16:19:14.282539Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "BwY7StuMkuV4"
   },
   "outputs": [],
   "source": [
    "eval_policy = agent.policy\n",
    "collect_policy = agent.collect_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2Qs1Fl3dV0ae"
   },
   "source": [
    "Policies can be created independently of agents. For example, use `tf_agents.policies.random_tf_policy` to create a policy which will randomly select an action for each `time_step`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T16:19:15.530035Z",
     "start_time": "2019-11-13T16:19:15.526319Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "HE37-UCIrE69"
   },
   "outputs": [],
   "source": [
    "random_policy = random_tf_policy.RandomTFPolicy(train_env.time_step_spec(),\n",
    "                                                train_env.action_spec())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dOlnlRRsUbxP"
   },
   "source": [
    "To get an action from a policy, call the `policy.action(time_step)` method. The `time_step` contains the observation from the environment. This method returns a `PolicyStep`, which is a named tuple with three components:\n",
    "\n",
    "-   `action` — the action to be taken (in this case, `0` or `1`)\n",
    "-   `state` — used for stateful (that is, RNN-based) policies\n",
    "-   `info` — auxiliary data, such as log probabilities of actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T16:19:24.673261Z",
     "start_time": "2019-11-13T16:19:24.667777Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "5gCcpXswVAxk"
   },
   "outputs": [],
   "source": [
    "example_environment = tf_py_environment.TFPyEnvironment(\n",
    "    suite_gym.load('CartPole-v0'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T16:19:28.230383Z",
     "start_time": "2019-11-13T16:19:28.225030Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "D4DHZtq3Ndis"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TimeStep(step_type=<tf.Tensor: id=205, shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, reward=<tf.Tensor: id=206, shape=(1,), dtype=float32, numpy=array([0.], dtype=float32)>, discount=<tf.Tensor: id=207, shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: id=208, shape=(1, 4), dtype=float32, numpy=\n",
       "array([[-0.01637382, -0.02537851, -0.0332469 , -0.01488001]],\n",
       "      dtype=float32)>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_step = example_environment.reset() # init the episode\n",
    "time_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T16:19:29.743688Z",
     "start_time": "2019-11-13T16:19:29.731485Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "PRFqAUzpNaAW"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PolicyStep(action=<tf.Tensor: id=223, shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=())"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_policy.action(time_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "94rCXQtbUbXv"
   },
   "source": [
    "## Metrics and Evaluation\n",
    "\n",
    "The most common metric used to evaluate a policy is the average return. The return is the sum of rewards obtained while running a policy in an environment for an episode. Several episodes are run, creating an average return.\n",
    "\n",
    "The following function computes the average return of a policy, given the policy, environment, and a number of episodes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T16:21:11.329755Z",
     "start_time": "2019-11-13T16:21:11.324133Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "bitzHo5_UbXy"
   },
   "outputs": [],
   "source": [
    "# @test {\"skip\": true}\n",
    "def compute_avg_return(environment, policy, num_episodes=10):\n",
    "\n",
    "    total_return = 0.0\n",
    "    for _ in range(num_episodes):\n",
    "\n",
    "        time_step = environment.reset()\n",
    "        episode_return = 0.0\n",
    "\n",
    "        while not time_step.is_last():\n",
    "            action_step = policy.action(time_step)\n",
    "            time_step = environment.step(action_step.action)\n",
    "            episode_return += time_step.reward\n",
    "        total_return += episode_return\n",
    "\n",
    "    avg_return = total_return / num_episodes\n",
    "    return avg_return.numpy()[0]\n",
    "\n",
    "\n",
    "# See also the metrics module for standard implementations of different metrics.\n",
    "# https://github.com/tensorflow/agents/tree/master/tf_agents/metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_snCVvq5Z8lJ"
   },
   "source": [
    "Running this computation on the `random_policy` shows a baseline performance in the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T16:21:12.396842Z",
     "start_time": "2019-11-13T16:21:12.090042Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "9bgU6Q6BZ8Bp"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27.0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_avg_return(eval_env, random_policy, num_eval_episodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NLva6g2jdWgr"
   },
   "source": [
    "## Replay Buffer\n",
    "\n",
    "The replay buffer keeps track of data collected from the environment. This tutorial uses `tf_agents.replay_buffers.tf_uniform_replay_buffer.TFUniformReplayBuffer`, as it is the most common. \n",
    "\n",
    "The constructor requires the specs for the data it will be collecting. This is available from the agent using the `collect_data_spec` method. The batch size and maximum buffer length are also required.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T16:21:37.261104Z",
     "start_time": "2019-11-13T16:21:37.241916Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "vX2zGUWJGWAl"
   },
   "outputs": [],
   "source": [
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "    data_spec=agent.collect_data_spec,\n",
    "    batch_size=train_env.batch_size,\n",
    "    max_length=replay_buffer_max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZGNTDJpZs4NN"
   },
   "source": [
    "For most agents, `collect_data_spec` is a named tuple called `Trajectory`, containing the specs for observations, actions, rewards, and other items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T16:21:37.869118Z",
     "start_time": "2019-11-13T16:21:37.865414Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "sy6g1tGcfRlw"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('step_type',\n",
       " 'observation',\n",
       " 'action',\n",
       " 'policy_info',\n",
       " 'next_step_type',\n",
       " 'reward',\n",
       " 'discount')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.collect_data_spec._fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T16:21:38.164661Z",
     "start_time": "2019-11-13T16:21:38.160964Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "0rkaEAzBokKm"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_env.batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rVD5nQ9ZGo8_"
   },
   "source": [
    "## Data Collection\n",
    "\n",
    "Now execute the random policy in the environment for a few steps, recording the data in the replay buffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T16:22:19.796716Z",
     "start_time": "2019-11-13T16:22:19.581326Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "wr1KSAEGG4h9"
   },
   "outputs": [],
   "source": [
    "# @test {\"skip\": true}\n",
    "def collect_step(environment, policy, buffer):\n",
    "    time_step = environment.current_time_step()\n",
    "    action_step = policy.action(time_step)\n",
    "    next_time_step = environment.step(action_step.action)\n",
    "    traj = trajectory.from_transition(time_step, action_step, next_time_step)\n",
    "\n",
    "    # Add trajectory to the replay buffer\n",
    "    buffer.add_batch(traj)\n",
    "\n",
    "\n",
    "def collect_data(env, policy, buffer, steps):\n",
    "    for _ in range(steps):\n",
    "        collect_step(env, policy, buffer)\n",
    "\n",
    "\n",
    "collect_data(train_env, random_policy, replay_buffer, steps=100)\n",
    "\n",
    "# This loop is so common in RL, that we provide standard implementations.\n",
    "# For more details see the drivers module.\n",
    "# https://github.com/tensorflow/agents/blob/master/tf_agents/docs/python/tf_agents/drivers.md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "84z5pQJdoKxo"
   },
   "source": [
    "The replay buffer is now a collection of Trajectories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T16:25:04.220905Z",
     "start_time": "2019-11-13T16:25:04.084281Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "4wZnLu2ViO4E"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Trajectory(step_type=<tf.Tensor: id=20379, shape=(), dtype=int32, numpy=1>, observation=<tf.Tensor: id=20380, shape=(4,), dtype=float32, numpy=array([-0.00138605,  0.36057106, -0.03063597, -0.5554334 ], dtype=float32)>, action=<tf.Tensor: id=20381, shape=(), dtype=int64, numpy=1>, policy_info=(), next_step_type=<tf.Tensor: id=20382, shape=(), dtype=int32, numpy=1>, reward=<tf.Tensor: id=20383, shape=(), dtype=float32, numpy=1.0>, discount=<tf.Tensor: id=20384, shape=(), dtype=float32, numpy=1.0>),\n",
       " BufferInfo(ids=<tf.Tensor: id=20385, shape=(), dtype=int64, numpy=4>, probabilities=<tf.Tensor: id=20386, shape=(), dtype=float32, numpy=0.01>))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For the curious:\n",
    "# Uncomment to peel one of these off and inspect it.\n",
    "iter(replay_buffer.as_dataset()).next()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TujU-PMUsKjS"
   },
   "source": [
    "The agent needs access to the replay buffer. This is provided by creating an iterable `tf.data.Dataset` pipeline which will feed data to the agent.\n",
    "\n",
    "Each row of the replay buffer only stores a single observation step. But since the DQN Agent needs both the current and next observation to compute the loss, the dataset pipeline will sample two adjacent rows for each item in the batch (`num_steps=2`).\n",
    "\n",
    "This dataset is also optimized by running parallel calls and prefetching data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T16:34:05.270419Z",
     "start_time": "2019-11-13T16:34:05.168578Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "ba7bilizt_qW"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<DatasetV1Adapter shapes: (Trajectory(step_type=(64, 2), observation=(64, 2, 4), action=(64, 2), policy_info=(), next_step_type=(64, 2), reward=(64, 2), discount=(64, 2)), BufferInfo(ids=(64, 2), probabilities=(64,))), types: (Trajectory(step_type=tf.int32, observation=tf.float32, action=tf.int64, policy_info=(), next_step_type=tf.int32, reward=tf.float32, discount=tf.float32), BufferInfo(ids=tf.int64, probabilities=tf.float32))>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dataset generates trajectories with shape [Bx2x...]\n",
    "dataset = replay_buffer.as_dataset(\n",
    "    num_parallel_calls=3, \n",
    "    sample_batch_size=batch_size, \n",
    "    num_steps=2).prefetch(3)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T16:34:06.587163Z",
     "start_time": "2019-11-13T16:34:06.505387Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "K13AST-2ppOq"
   },
   "outputs": [],
   "source": [
    "iterator = iter(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T16:34:14.056599Z",
     "start_time": "2019-11-13T16:34:14.035148Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "Th5w5Sff0b16"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Trajectory(step_type=<tf.Tensor: id=20548, shape=(64, 2), dtype=int32, numpy=\n",
       " array([[1, 1],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [0, 1],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [1, 2],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [2, 0],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [2, 0],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [1, 2],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [0, 1],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [1, 2],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [2, 0],\n",
       "        [1, 1],\n",
       "        [0, 1],\n",
       "        [1, 1],\n",
       "        [0, 1]], dtype=int32)>, observation=<tf.Tensor: id=20549, shape=(64, 2, 4), dtype=float32, numpy=\n",
       " array([[[-7.52777886e-03, -1.71522051e-01, -1.36831065e-03,\n",
       "           3.21365774e-01],\n",
       "         [-1.09582199e-02,  2.36193649e-02,  5.05900523e-03,\n",
       "           2.82516629e-02]],\n",
       " \n",
       "        [[-1.59024924e-01, -2.54192382e-01,  1.82730064e-01,\n",
       "           5.48328876e-01],\n",
       "         [-1.64108768e-01, -6.20440431e-02,  1.93696633e-01,\n",
       "           3.18329096e-01]],\n",
       " \n",
       "        [[-3.84148620e-02, -2.07806990e-01,  1.53745618e-03,\n",
       "           2.82447278e-01],\n",
       "         [-4.25710008e-02, -1.27070025e-02,  7.18640210e-03,\n",
       "          -9.75033361e-03]],\n",
       " \n",
       "        [[ 4.01636921e-02,  5.70557237e-01,  1.01940567e-02,\n",
       "          -8.58476460e-01],\n",
       "         [ 5.15748337e-02,  3.75297904e-01, -6.97547197e-03,\n",
       "          -5.62605679e-01]],\n",
       " \n",
       "        [[ 5.82537334e-03,  5.56109428e-01, -4.17446345e-02,\n",
       "          -8.57608914e-01],\n",
       "         [ 1.69475619e-02,  3.61580342e-01, -5.88968135e-02,\n",
       "          -5.78338802e-01]],\n",
       " \n",
       "        [[ 2.93021146e-02, -1.36916256e-02,  2.75216755e-02,\n",
       "          -4.90788044e-03],\n",
       "         [ 2.90282816e-02,  1.81025028e-01,  2.74235178e-02,\n",
       "          -2.88781911e-01]],\n",
       " \n",
       "        [[-7.72844488e-03, -4.78690341e-02, -3.06480769e-02,\n",
       "           1.96728797e-05],\n",
       "         [-8.68582539e-03, -2.42538348e-01, -3.06476839e-02,\n",
       "           2.82877386e-01]],\n",
       " \n",
       "        [[ 4.89591844e-02, -2.08894946e-02, -1.18084490e-01,\n",
       "          -1.64721817e-01],\n",
       "         [ 4.85413931e-02,  1.75707489e-01, -1.21378928e-01,\n",
       "          -4.92199898e-01]],\n",
       " \n",
       "        [[ 3.81444655e-02, -4.03813988e-01,  1.17691327e-02,\n",
       "           5.77788472e-01],\n",
       "         [ 3.00681870e-02, -5.99098861e-01,  2.33249031e-02,\n",
       "           8.74155641e-01]],\n",
       " \n",
       "        [[ 5.17053008e-02,  1.79216430e-01, -1.36024937e-01,\n",
       "          -5.71125448e-01],\n",
       "         [ 5.52896298e-02,  3.75957340e-01, -1.47447452e-01,\n",
       "          -9.03378904e-01]],\n",
       " \n",
       "        [[-7.52777886e-03, -1.71522051e-01, -1.36831065e-03,\n",
       "           3.21365774e-01],\n",
       "         [-1.09582199e-02,  2.36193649e-02,  5.05900523e-03,\n",
       "           2.82516629e-02]],\n",
       " \n",
       "        [[-3.94371115e-02, -2.08045855e-01,  8.82699154e-04,\n",
       "           2.87716985e-01],\n",
       "         [-4.35980298e-02, -1.29365074e-02,  6.63703913e-03,\n",
       "          -4.68741171e-03]],\n",
       " \n",
       "        [[-3.81218195e-02, -6.04707003e-01,  1.27004102e-01,\n",
       "           1.00467658e+00],\n",
       "         [-5.02159595e-02, -4.11489040e-01,  1.47097632e-01,\n",
       "           7.54422903e-01]],\n",
       " \n",
       "        [[-1.15716003e-01, -6.36312425e-01,  1.13551177e-01,\n",
       "           9.51135874e-01],\n",
       "         [-1.28442243e-01, -4.42886710e-01,  1.32573888e-01,\n",
       "           6.96178138e-01]],\n",
       " \n",
       "        [[-3.91789190e-02, -1.29097495e-02,  9.88250715e-04,\n",
       "          -5.27757918e-03],\n",
       "         [-3.94371115e-02, -2.08045855e-01,  8.82699154e-04,\n",
       "           2.87716985e-01]],\n",
       " \n",
       "        [[-4.28251401e-02,  1.82311147e-01,  6.99139526e-03,\n",
       "          -3.00157219e-01],\n",
       "         [-3.91789190e-02, -1.29097495e-02,  9.88250715e-04,\n",
       "          -5.27757918e-03]],\n",
       " \n",
       "        [[-1.97029375e-02,  3.60341966e-01, -2.75667571e-03,\n",
       "          -5.50207198e-01],\n",
       "         [-1.24960989e-02,  5.55502534e-01, -1.37608200e-02,\n",
       "          -8.43757391e-01]],\n",
       " \n",
       "        [[-1.65349647e-01, -2.59321064e-01,  2.00063214e-01,\n",
       "           6.65302217e-01],\n",
       "         [-1.70536071e-01, -4.56580400e-01,  2.13369265e-01,\n",
       "           1.01371646e+00]],\n",
       " \n",
       "        [[-7.76777714e-02, -8.28135133e-01,  5.97599894e-02,\n",
       "           1.16724432e+00],\n",
       "         [-9.42404717e-02, -6.33839488e-01,  8.31048787e-02,\n",
       "           8.93880129e-01]],\n",
       " \n",
       "        [[-1.79189742e-02, -6.01935029e-01,  9.46321413e-02,\n",
       "           9.40054417e-01],\n",
       "         [-2.99576744e-02, -4.08207238e-01,  1.13433234e-01,\n",
       "           6.78543270e-01]],\n",
       " \n",
       "        [[-1.13038927e-01, -6.03273869e-01,  1.10840879e-01,\n",
       "           9.88093972e-01],\n",
       "         [-1.25104398e-01, -4.09796387e-01,  1.30602762e-01,\n",
       "           7.32179224e-01]],\n",
       " \n",
       "        [[-2.22807936e-02, -6.31972671e-01, -1.36753600e-02,\n",
       "           8.50445151e-01],\n",
       "         [-3.49202454e-02, -4.36666936e-01,  3.33354366e-03,\n",
       "           5.53493559e-01]],\n",
       " \n",
       "        [[-1.70536071e-01, -4.56580400e-01,  2.13369265e-01,\n",
       "           1.01371646e+00],\n",
       "         [-7.99921528e-03,  2.35718247e-02, -1.95431290e-03,\n",
       "           2.93001086e-02]],\n",
       " \n",
       "        [[-6.50285557e-02, -6.32460594e-01,  4.25300300e-02,\n",
       "           8.61497939e-01],\n",
       "         [-7.76777714e-02, -8.28135133e-01,  5.97599894e-02,\n",
       "           1.16724432e+00]],\n",
       " \n",
       "        [[ 3.53286900e-02,  3.63377512e-01, -8.82431641e-02,\n",
       "          -6.19252384e-01],\n",
       "         [ 4.25962433e-02,  1.69591680e-01, -1.00628212e-01,\n",
       "          -3.55614483e-01]],\n",
       " \n",
       "        [[-9.80362389e-03, -4.05767560e-01,  8.21788833e-02,\n",
       "           6.22662902e-01],\n",
       "         [-1.79189742e-02, -6.01935029e-01,  9.46321413e-02,\n",
       "           9.40054417e-01]],\n",
       " \n",
       "        [[ 3.26487832e-02,  3.75745416e-01,  2.16478799e-02,\n",
       "          -5.72691143e-01],\n",
       "         [ 4.01636921e-02,  5.70557237e-01,  1.01940567e-02,\n",
       "          -8.58476460e-01]],\n",
       " \n",
       "        [[-1.04900114e-01, -4.06940609e-01,  9.75134894e-02,\n",
       "           6.66369617e-01],\n",
       "         [-1.13038927e-01, -6.03273869e-01,  1.10840879e-01,\n",
       "           9.88093972e-01]],\n",
       " \n",
       "        [[ 4.59880754e-02, -2.39662994e-02, -1.07740499e-01,\n",
       "          -9.62810442e-02],\n",
       "         [ 4.55087498e-02,  1.72521666e-01, -1.09666124e-01,\n",
       "          -4.20918465e-01]],\n",
       " \n",
       "        [[-7.69760087e-02, -7.95289993e-01,  5.46815172e-02,\n",
       "           1.20832825e+00],\n",
       "         [-9.28818062e-02, -6.00915313e-01,  7.88480788e-02,\n",
       "           9.33270395e-01]],\n",
       " \n",
       "        [[-5.27203120e-02, -2.08856747e-01,  1.83998775e-02,\n",
       "           3.05640966e-01],\n",
       "         [-5.68974465e-02, -4.04235989e-01,  2.45126970e-02,\n",
       "           6.04069471e-01]],\n",
       " \n",
       "        [[ 8.18650946e-02,  5.77164769e-01, -2.10324600e-01,\n",
       "          -1.34783983e+00],\n",
       "         [-4.15645242e-02, -1.24302562e-02,  8.18920135e-03,\n",
       "          -1.58555117e-02]],\n",
       " \n",
       "        [[ 1.80862080e-02, -7.94530094e-01,  4.08080146e-02,\n",
       "           1.17407954e+00],\n",
       "         [ 2.19560717e-03, -5.99961519e-01,  6.42896071e-02,\n",
       "           8.94463956e-01]],\n",
       " \n",
       "        [[-4.36535850e-02, -6.31835520e-01,  1.44034149e-02,\n",
       "           8.47224891e-01],\n",
       "         [-5.62902950e-02, -4.36913013e-01,  3.13479118e-02,\n",
       "           5.59105873e-01]],\n",
       " \n",
       "        [[-9.80362389e-03, -4.05767560e-01,  8.21788833e-02,\n",
       "           6.22662902e-01],\n",
       "         [-1.79189742e-02, -6.01935029e-01,  9.46321413e-02,\n",
       "           9.40054417e-01]],\n",
       " \n",
       "        [[ 2.19560717e-03, -5.99961519e-01,  6.42896071e-02,\n",
       "           8.94463956e-01],\n",
       "         [-9.80362389e-03, -4.05767560e-01,  8.21788833e-02,\n",
       "           6.22662902e-01]],\n",
       " \n",
       "        [[-1.97029375e-02,  3.60341966e-01, -2.75667571e-03,\n",
       "          -5.50207198e-01],\n",
       "         [-1.24960989e-02,  5.55502534e-01, -1.37608200e-02,\n",
       "          -8.43757391e-01]],\n",
       " \n",
       "        [[-1.15716003e-01, -6.36312425e-01,  1.13551177e-01,\n",
       "           9.51135874e-01],\n",
       "         [-1.28442243e-01, -4.42886710e-01,  1.32573888e-01,\n",
       "           6.96178138e-01]],\n",
       " \n",
       "        [[-6.49821684e-02, -5.99692047e-01,  3.65940854e-02,\n",
       "           9.04371500e-01],\n",
       "         [-7.69760087e-02, -7.95289993e-01,  5.46815172e-02,\n",
       "           1.20832825e+00]],\n",
       " \n",
       "        [[-1.24960989e-02,  5.55502534e-01, -1.37608200e-02,\n",
       "          -8.43757391e-01],\n",
       "         [-1.38604792e-03,  3.60571057e-01, -3.06359679e-02,\n",
       "          -5.55433393e-01]],\n",
       " \n",
       "        [[ 7.42634907e-02,  3.80080402e-01, -1.90285757e-01,\n",
       "          -1.00194228e+00],\n",
       "         [ 8.18650946e-02,  5.77164769e-01, -2.10324600e-01,\n",
       "          -1.34783983e+00]],\n",
       " \n",
       "        [[-1.28442243e-01, -4.42886710e-01,  1.32573888e-01,\n",
       "           6.96178138e-01],\n",
       "         [-1.37299985e-01, -6.39573812e-01,  1.46497458e-01,\n",
       "           1.02748251e+00]],\n",
       " \n",
       "        [[-3.49202454e-02, -4.36666936e-01,  3.33354366e-03,\n",
       "           5.53493559e-01],\n",
       "         [-4.36535850e-02, -6.31835520e-01,  1.44034149e-02,\n",
       "           8.47224891e-01]],\n",
       " \n",
       "        [[-7.99921528e-03,  2.35718247e-02, -1.95431290e-03,\n",
       "           2.93001086e-02],\n",
       "         [-7.52777886e-03, -1.71522051e-01, -1.36831065e-03,\n",
       "           3.21365774e-01]],\n",
       " \n",
       "        [[-1.13038927e-01, -6.03273869e-01,  1.10840879e-01,\n",
       "           9.88093972e-01],\n",
       "         [-1.25104398e-01, -4.09796387e-01,  1.30602762e-01,\n",
       "           7.32179224e-01]],\n",
       " \n",
       "        [[-6.28191009e-02, -2.61576418e-02,  1.72414199e-01,\n",
       "           2.73903728e-01],\n",
       "         [-6.33422583e-02,  1.66138768e-01,  1.77892283e-01,\n",
       "           4.01747525e-02]],\n",
       " \n",
       "        [[ 6.28087744e-02,  5.72735667e-01, -1.65515020e-01,\n",
       "          -1.23853624e+00],\n",
       "         [ 7.42634907e-02,  3.80080402e-01, -1.90285757e-01,\n",
       "          -1.00194228e+00]],\n",
       " \n",
       "        [[ 4.85413931e-02,  1.75707489e-01, -1.21378928e-01,\n",
       "          -4.92199898e-01],\n",
       "         [ 5.20555414e-02, -1.75121035e-02, -1.31222934e-01,\n",
       "          -2.40100726e-01]],\n",
       " \n",
       "        [[-1.28442243e-01, -4.42886710e-01,  1.32573888e-01,\n",
       "           6.96178138e-01],\n",
       "         [-1.37299985e-01, -6.39573812e-01,  1.46497458e-01,\n",
       "           1.02748251e+00]],\n",
       " \n",
       "        [[-7.52777886e-03, -1.71522051e-01, -1.36831065e-03,\n",
       "           3.21365774e-01],\n",
       "         [-1.09582199e-02,  2.36193649e-02,  5.05900523e-03,\n",
       "           2.82516629e-02]],\n",
       " \n",
       "        [[-1.50091454e-01, -4.46673334e-01,  1.67047113e-01,\n",
       "           7.84147561e-01],\n",
       "         [-1.59024924e-01, -2.54192382e-01,  1.82730064e-01,\n",
       "           5.48328876e-01]],\n",
       " \n",
       "        [[-9.28818062e-02, -6.00915313e-01,  7.88480788e-02,\n",
       "           9.33270395e-01],\n",
       "         [-1.04900114e-01, -4.06940609e-01,  9.75134894e-02,\n",
       "           6.66369617e-01]],\n",
       " \n",
       "        [[-5.84457405e-02, -2.18668088e-01,  1.62186086e-01,\n",
       "           5.11405885e-01],\n",
       "         [-6.28191009e-02, -2.61576418e-02,  1.72414199e-01,\n",
       "           2.73903728e-01]],\n",
       " \n",
       "        [[-1.25104398e-01, -4.09796387e-01,  1.30602762e-01,\n",
       "           7.32179224e-01],\n",
       "         [-1.33300334e-01, -6.06457949e-01,  1.45246342e-01,\n",
       "           1.06294751e+00]],\n",
       " \n",
       "        [[-1.65349647e-01, -2.59321064e-01,  2.00063214e-01,\n",
       "           6.65302217e-01],\n",
       "         [-1.70536071e-01, -4.56580400e-01,  2.13369265e-01,\n",
       "           1.01371646e+00]],\n",
       " \n",
       "        [[ 5.15748337e-02,  3.75297904e-01, -6.97547197e-03,\n",
       "          -5.62605679e-01],\n",
       "         [ 5.90807945e-02,  1.80274546e-01, -1.82275865e-02,\n",
       "          -2.72128493e-01]],\n",
       " \n",
       "        [[-2.22807936e-02, -6.31972671e-01, -1.36753600e-02,\n",
       "           8.50445151e-01],\n",
       "         [-3.49202454e-02, -4.36666936e-01,  3.33354366e-03,\n",
       "           5.53493559e-01]],\n",
       " \n",
       "        [[-3.81616615e-02, -1.26599390e-02,  1.75322185e-03,\n",
       "          -1.07882833e-02],\n",
       "         [-3.84148620e-02, -2.07806990e-01,  1.53745618e-03,\n",
       "           2.82447278e-01]],\n",
       " \n",
       "        [[ 4.59880754e-02, -2.39662994e-02, -1.07740499e-01,\n",
       "          -9.62810442e-02],\n",
       "         [ 4.55087498e-02,  1.72521666e-01, -1.09666124e-01,\n",
       "          -4.20918465e-01]],\n",
       " \n",
       "        [[-1.70536071e-01, -4.56580400e-01,  2.13369265e-01,\n",
       "           1.01371646e+00],\n",
       "         [-7.99921528e-03,  2.35718247e-02, -1.95431290e-03,\n",
       "           2.93001086e-02]],\n",
       " \n",
       "        [[ 5.52896298e-02,  3.75957340e-01, -1.47447452e-01,\n",
       "          -9.03378904e-01],\n",
       "         [ 6.28087744e-02,  5.72735667e-01, -1.65515020e-01,\n",
       "          -1.23853624e+00]],\n",
       " \n",
       "        [[-7.99921528e-03,  2.35718247e-02, -1.95431290e-03,\n",
       "           2.93001086e-02],\n",
       "         [-7.52777886e-03, -1.71522051e-01, -1.36831065e-03,\n",
       "           3.21365774e-01]],\n",
       " \n",
       "        [[-5.27203120e-02, -2.08856747e-01,  1.83998775e-02,\n",
       "           3.05640966e-01],\n",
       "         [-5.68974465e-02, -4.04235989e-01,  2.45126970e-02,\n",
       "           6.04069471e-01]],\n",
       " \n",
       "        [[ 2.59186700e-02, -1.27214352e-02,  3.42085473e-02,\n",
       "          -2.63235681e-02],\n",
       "         [ 2.56642401e-02,  1.81893662e-01,  3.36820781e-02,\n",
       "          -3.08020085e-01]]], dtype=float32)>, action=<tf.Tensor: id=20550, shape=(64, 2), dtype=int64, numpy=\n",
       " array([[1, 1],\n",
       "        [1, 0],\n",
       "        [1, 1],\n",
       "        [0, 0],\n",
       "        [0, 1],\n",
       "        [1, 1],\n",
       "        [0, 0],\n",
       "        [1, 0],\n",
       "        [0, 0],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [1, 0],\n",
       "        [1, 1],\n",
       "        [1, 0],\n",
       "        [0, 1],\n",
       "        [0, 0],\n",
       "        [1, 0],\n",
       "        [0, 1],\n",
       "        [1, 1],\n",
       "        [1, 0],\n",
       "        [1, 0],\n",
       "        [1, 0],\n",
       "        [1, 0],\n",
       "        [0, 1],\n",
       "        [0, 0],\n",
       "        [0, 1],\n",
       "        [1, 0],\n",
       "        [0, 1],\n",
       "        [1, 0],\n",
       "        [1, 1],\n",
       "        [0, 0],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [1, 0],\n",
       "        [0, 1],\n",
       "        [1, 0],\n",
       "        [1, 0],\n",
       "        [1, 0],\n",
       "        [0, 1],\n",
       "        [0, 1],\n",
       "        [1, 1],\n",
       "        [0, 1],\n",
       "        [0, 1],\n",
       "        [0, 1],\n",
       "        [1, 0],\n",
       "        [1, 0],\n",
       "        [0, 1],\n",
       "        [0, 1],\n",
       "        [0, 1],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [1, 0],\n",
       "        [1, 1],\n",
       "        [0, 0],\n",
       "        [0, 1],\n",
       "        [0, 0],\n",
       "        [1, 0],\n",
       "        [0, 1],\n",
       "        [1, 0],\n",
       "        [1, 0],\n",
       "        [1, 0],\n",
       "        [0, 1],\n",
       "        [0, 0],\n",
       "        [1, 0]])>, policy_info=(), next_step_type=<tf.Tensor: id=20551, shape=(64, 2), dtype=int32, numpy=\n",
       " array([[1, 1],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [2, 0],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [0, 1],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [0, 1],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [2, 0],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [1, 2],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [2, 0],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [0, 1],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [1, 1]], dtype=int32)>, reward=<tf.Tensor: id=20552, shape=(64, 2), dtype=float32, numpy=\n",
       " array([[1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 0.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [0., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [0., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 0.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 0.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [0., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.]], dtype=float32)>, discount=<tf.Tensor: id=20553, shape=(64, 2), dtype=float32, numpy=\n",
       " array([[1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [0., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [0., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 0.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [0., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.]], dtype=float32)>),\n",
       " BufferInfo(ids=<tf.Tensor: id=20554, shape=(64, 2), dtype=int64, numpy=\n",
       " array([[96, 97],\n",
       "        [91, 92],\n",
       "        [23, 24],\n",
       "        [50, 51],\n",
       "        [ 5,  6],\n",
       "        [47, 48],\n",
       "        [76, 77],\n",
       "        [12, 13],\n",
       "        [57, 58],\n",
       "        [15, 16],\n",
       "        [96, 97],\n",
       "        [27, 28],\n",
       "        [64, 65],\n",
       "        [87, 88],\n",
       "        [26, 27],\n",
       "        [25, 26],\n",
       "        [ 2,  3],\n",
       "        [93, 94],\n",
       "        [84, 85],\n",
       "        [62, 63],\n",
       "        [39, 40],\n",
       "        [79, 80],\n",
       "        [94, 95],\n",
       "        [83, 84],\n",
       "        [ 8,  9],\n",
       "        [61, 62],\n",
       "        [49, 50],\n",
       "        [38, 39],\n",
       "        [10, 11],\n",
       "        [36, 37],\n",
       "        [33, 34],\n",
       "        [19, 20],\n",
       "        [59, 60],\n",
       "        [81, 82],\n",
       "        [61, 62],\n",
       "        [60, 61],\n",
       "        [ 2,  3],\n",
       "        [87, 88],\n",
       "        [35, 36],\n",
       "        [ 3,  4],\n",
       "        [18, 19],\n",
       "        [88, 89],\n",
       "        [80, 81],\n",
       "        [95, 96],\n",
       "        [39, 40],\n",
       "        [67, 68],\n",
       "        [17, 18],\n",
       "        [13, 14],\n",
       "        [88, 89],\n",
       "        [96, 97],\n",
       "        [90, 91],\n",
       "        [37, 38],\n",
       "        [66, 67],\n",
       "        [40, 41],\n",
       "        [93, 94],\n",
       "        [51, 52],\n",
       "        [79, 80],\n",
       "        [22, 23],\n",
       "        [10, 11],\n",
       "        [94, 95],\n",
       "        [16, 17],\n",
       "        [95, 96],\n",
       "        [33, 34],\n",
       "        [45, 46]])>, probabilities=<tf.Tensor: id=20555, shape=(64,), dtype=float32, numpy=\n",
       " array([0.01010101, 0.01010101, 0.01010101, 0.01010101, 0.01010101,\n",
       "        0.01010101, 0.01010101, 0.01010101, 0.01010101, 0.01010101,\n",
       "        0.01010101, 0.01010101, 0.01010101, 0.01010101, 0.01010101,\n",
       "        0.01010101, 0.01010101, 0.01010101, 0.01010101, 0.01010101,\n",
       "        0.01010101, 0.01010101, 0.01010101, 0.01010101, 0.01010101,\n",
       "        0.01010101, 0.01010101, 0.01010101, 0.01010101, 0.01010101,\n",
       "        0.01010101, 0.01010101, 0.01010101, 0.01010101, 0.01010101,\n",
       "        0.01010101, 0.01010101, 0.01010101, 0.01010101, 0.01010101,\n",
       "        0.01010101, 0.01010101, 0.01010101, 0.01010101, 0.01010101,\n",
       "        0.01010101, 0.01010101, 0.01010101, 0.01010101, 0.01010101,\n",
       "        0.01010101, 0.01010101, 0.01010101, 0.01010101, 0.01010101,\n",
       "        0.01010101, 0.01010101, 0.01010101, 0.01010101, 0.01010101,\n",
       "        0.01010101, 0.01010101, 0.01010101, 0.01010101], dtype=float32)>))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For the curious:\n",
    "# Uncomment to see what the dataset iterator is feeding to the agent.\n",
    "# Compare this representation of replay data \n",
    "# to the collection of individual trajectories shown earlier.\n",
    "\n",
    "iterator.next()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hBc9lj9VWWtZ"
   },
   "source": [
    "## Training the agent\n",
    "\n",
    "Two thing must happen during the training loop:\n",
    "\n",
    "-   collect data from the environment\n",
    "-   use that data to train the agent's neural network(s)\n",
    "\n",
    "This example also periodicially evaluates the policy and prints the current score.\n",
    "\n",
    "The following will take ~5 minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T16:35:33.709045Z",
     "start_time": "2019-11-13T16:35:32.171516Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "0pTbJ3PeyF-u"
   },
   "outputs": [],
   "source": [
    "#@test {\"skip\": true}\n",
    "# %%time\n",
    "\n",
    "# (Optional) Optimize by wrapping some of the code in a graph using TF function.\n",
    "agent.train = common.function(agent.train)\n",
    "\n",
    "# Reset the train step\n",
    "agent.train_step_counter.assign(0)\n",
    "\n",
    "# Evaluate the agent's policy once before training.\n",
    "avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n",
    "returns = [avg_return]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T16:35:33.716444Z",
     "start_time": "2019-11-13T16:35:33.711673Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[49.9]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T16:39:52.897240Z",
     "start_time": "2019-11-13T16:36:09.402861Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "0pTbJ3PeyF-u",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 200: loss = 8.753681182861328\n",
      "step = 400: loss = 11.245573997497559\n",
      "step = 600: loss = 40.863929748535156\n",
      "step = 800: loss = 11.352370262145996\n",
      "step = 1000: loss = 65.85738372802734\n",
      "step = 1000: Average Return = 200.0\n",
      "step = 1200: loss = 135.07044982910156\n",
      "step = 1400: loss = 93.34333038330078\n",
      "step = 1600: loss = 477.7996520996094\n",
      "step = 1800: loss = 31.08798599243164\n",
      "step = 2000: loss = 285.2479248046875\n",
      "step = 2000: Average Return = 19.5\n",
      "step = 2200: loss = 432.3360595703125\n",
      "step = 2400: loss = 100.208984375\n",
      "step = 2600: loss = 332.1755676269531\n",
      "step = 2800: loss = 152.61410522460938\n",
      "step = 3000: loss = 68.66222381591797\n",
      "step = 3000: Average Return = 55.20000076293945\n",
      "step = 3200: loss = 22.916763305664062\n",
      "step = 3400: loss = 193.3721923828125\n",
      "step = 3600: loss = 181.49676513671875\n",
      "step = 3800: loss = 301.90875244140625\n",
      "step = 4000: loss = 105.87191772460938\n",
      "step = 4000: Average Return = 76.80000305175781\n",
      "step = 4200: loss = 6.239327430725098\n",
      "step = 4400: loss = 39.701255798339844\n",
      "step = 4600: loss = 272.8031311035156\n",
      "step = 4800: loss = 301.5104064941406\n",
      "step = 5000: loss = 10.86923885345459\n",
      "step = 5000: Average Return = 82.30000305175781\n",
      "step = 5200: loss = 275.66668701171875\n",
      "step = 5400: loss = 601.7164306640625\n",
      "step = 5600: loss = 5.033618927001953\n",
      "step = 5800: loss = 18.604238510131836\n",
      "step = 6000: loss = 3.671659469604492\n",
      "step = 6000: Average Return = 102.5\n",
      "step = 6200: loss = 64.57603454589844\n",
      "step = 6400: loss = 232.62347412109375\n",
      "step = 6600: loss = 233.93106079101562\n",
      "step = 6800: loss = 37.44921875\n",
      "step = 7000: loss = 67.24632263183594\n",
      "step = 7000: Average Return = 114.80000305175781\n",
      "step = 7200: loss = 238.44387817382812\n",
      "step = 7400: loss = 67.2303466796875\n",
      "step = 7600: loss = 34.002628326416016\n",
      "step = 7800: loss = 377.4160461425781\n",
      "step = 8000: loss = 8.556896209716797\n",
      "step = 8000: Average Return = 164.89999389648438\n",
      "step = 8200: loss = 6.342598915100098\n",
      "step = 8400: loss = 473.46026611328125\n",
      "step = 8600: loss = 43.22121810913086\n",
      "step = 8800: loss = 42.506065368652344\n",
      "step = 9000: loss = 10.111870765686035\n",
      "step = 9000: Average Return = 130.5\n",
      "step = 9200: loss = 7.977750778198242\n",
      "step = 9400: loss = 8.082757949829102\n",
      "step = 9600: loss = 286.2572021484375\n",
      "step = 9800: loss = 528.6027221679688\n",
      "step = 10000: loss = 14.315174102783203\n",
      "step = 10000: Average Return = 200.0\n",
      "step = 10200: loss = 10.96270751953125\n",
      "step = 10400: loss = 578.8799438476562\n",
      "step = 10600: loss = 266.91485595703125\n",
      "step = 10800: loss = 422.3280944824219\n",
      "step = 11000: loss = 9.56598949432373\n",
      "step = 11000: Average Return = 200.0\n",
      "step = 11200: loss = 436.84490966796875\n",
      "step = 11400: loss = 9.25899887084961\n",
      "step = 11600: loss = 12.86512565612793\n",
      "step = 11800: loss = 13.260700225830078\n",
      "step = 12000: loss = 12.676994323730469\n",
      "step = 12000: Average Return = 200.0\n",
      "step = 12200: loss = 222.7823944091797\n",
      "step = 12400: loss = 22.052309036254883\n",
      "step = 12600: loss = 11.846909523010254\n",
      "step = 12800: loss = 403.3575439453125\n",
      "step = 13000: loss = 15.212924003601074\n",
      "step = 13000: Average Return = 200.0\n",
      "step = 13200: loss = 1209.80517578125\n",
      "step = 13400: loss = 18.76932144165039\n",
      "step = 13600: loss = 465.1620788574219\n",
      "step = 13800: loss = 1388.6611328125\n",
      "step = 14000: loss = 21.511554718017578\n",
      "step = 14000: Average Return = 200.0\n",
      "step = 14200: loss = 22.452878952026367\n",
      "step = 14400: loss = 526.8831787109375\n",
      "step = 14600: loss = 1740.147705078125\n",
      "step = 14800: loss = 210.23562622070312\n",
      "step = 15000: loss = 3244.8232421875\n",
      "step = 15000: Average Return = 200.0\n",
      "step = 15200: loss = 1227.4403076171875\n",
      "step = 15400: loss = 454.3450622558594\n",
      "step = 15600: loss = 2039.31396484375\n",
      "step = 15800: loss = 32.99473571777344\n",
      "step = 16000: loss = 948.130126953125\n",
      "step = 16000: Average Return = 200.0\n",
      "step = 16200: loss = 37.08995819091797\n",
      "step = 16400: loss = 24.967716217041016\n",
      "step = 16600: loss = 1785.66796875\n",
      "step = 16800: loss = 806.6732177734375\n",
      "step = 17000: loss = 628.8943481445312\n",
      "step = 17000: Average Return = 200.0\n",
      "step = 17200: loss = 33.5939826965332\n",
      "step = 17400: loss = 1870.567138671875\n",
      "step = 17600: loss = 1959.753662109375\n",
      "step = 17800: loss = 47.42382049560547\n",
      "step = 18000: loss = 676.5200805664062\n",
      "step = 18000: Average Return = 200.0\n",
      "step = 18200: loss = 35.585819244384766\n",
      "step = 18400: loss = 46.946861267089844\n",
      "step = 18600: loss = 862.4982299804688\n",
      "step = 18800: loss = 37.66919708251953\n",
      "step = 19000: loss = 167.90997314453125\n",
      "step = 19000: Average Return = 200.0\n",
      "step = 19200: loss = 58.760498046875\n",
      "step = 19400: loss = 1379.970703125\n",
      "step = 19600: loss = 61.294490814208984\n",
      "step = 19800: loss = 1346.307861328125\n",
      "step = 20000: loss = 1114.1341552734375\n",
      "step = 20000: Average Return = 200.0\n"
     ]
    }
   ],
   "source": [
    "for _ in range(num_iterations):\n",
    "\n",
    "    # Collect a few steps using collect_policy and save to the replay buffer.\n",
    "    for _ in range(collect_steps_per_iteration):\n",
    "        collect_step(train_env, agent.collect_policy, replay_buffer)\n",
    "\n",
    "    # Sample a batch of data from the buffer and update the agent's network.\n",
    "    experience, unused_info = next(iterator)\n",
    "    train_loss = agent.train(experience).loss\n",
    "\n",
    "    step = agent.train_step_counter.numpy()\n",
    "\n",
    "    if step % log_interval == 0:\n",
    "        print('step = {0}: loss = {1}'.format(step, train_loss))\n",
    "\n",
    "    if step % eval_interval == 0:\n",
    "        avg_return = compute_avg_return(\n",
    "            eval_env, agent.policy, num_eval_episodes)\n",
    "        print('step = {0}: Average Return = {1}'.format(step, avg_return))\n",
    "        returns.append(avg_return)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "68jNcA_TiJDq"
   },
   "source": [
    "## Visualization\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aO-LWCdbbOIC"
   },
   "source": [
    "### Plots\n",
    "\n",
    "Use `matplotlib.pyplot` to chart how the policy improved during training.\n",
    "\n",
    "One iteration of `Cartpole-v0` consists of 200 time steps. The environment gives a reward of `+1` for each step the pole stays up, so the maximum return for one episode is 200. The charts shows the return increasing towards that maximum each time it is evaluated during training. (It may be a little unstable and not increase monotonically each time.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T16:40:14.049909Z",
     "start_time": "2019-11-13T16:40:13.804254Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "NxtL1mbOYCVO"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10.475, 250)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#@test {\"skip\": true}\n",
    "\n",
    "iterations = range(0, num_iterations + 1, eval_interval)\n",
    "plt.plot(iterations, returns)\n",
    "plt.ylabel('Average Return')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylim(top=250)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M7-XpPP99Cy7"
   },
   "source": [
    "### Videos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9pGfGxSH32gn"
   },
   "source": [
    "Charts are nice. But more exciting is seeing an agent actually performing a task in an environment. \n",
    "\n",
    "First, create a function to embed videos in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ULaGr8pvOKbl"
   },
   "outputs": [],
   "source": [
    "def embed_mp4(filename):\n",
    "  \"\"\"Embeds an mp4 file in the notebook.\"\"\"\n",
    "  video = open(filename,'rb').read()\n",
    "  b64 = base64.b64encode(video)\n",
    "  tag = '''\n",
    "  <video width=\"640\" height=\"480\" controls>\n",
    "    <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\">\n",
    "  Your browser does not support the video tag.\n",
    "  </video>'''.format(b64.decode())\n",
    "\n",
    "  return IPython.display.HTML(tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9c_PH-pX4Pr5"
   },
   "source": [
    "Now iterate through a few episodes of the Cartpole game with the agent. The underlying Python environment (the one \"inside\" the TensorFlow environment wrapper) provides a `render()` method, which outputs an image of the environment state. These can be collected into a video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "owOVWB158NlF"
   },
   "outputs": [],
   "source": [
    "def create_policy_eval_video(policy, filename, num_episodes=5, fps=30):\n",
    "  filename = filename + \".mp4\"\n",
    "  with imageio.get_writer(filename, fps=fps) as video:\n",
    "    for _ in range(num_episodes):\n",
    "      time_step = eval_env.reset()\n",
    "      video.append_data(eval_py_env.render())\n",
    "      while not time_step.is_last():\n",
    "        action_step = policy.action(time_step)\n",
    "        time_step = eval_env.step(action_step.action)\n",
    "        video.append_data(eval_py_env.render())\n",
    "  return embed_mp4(filename)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "create_policy_eval_video(agent.policy, \"trained-agent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "povaAOcZygLw"
   },
   "source": [
    "For fun, compare the trained agent (above) to an agent moving randomly. (It does not do as well.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pJZIdC37yNH4"
   },
   "outputs": [],
   "source": [
    "create_policy_eval_video(random_policy, \"random-agent\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "DQN Tutorial.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
